- 08:00 - Weekly TechPS Meeting #work/meeting
  collapsed:: true
	- Removed deploying and smoke testing lanes from sprints
	- Everyone is being added to backlog refinement meetings now
	- Dexter w/ Debezium and Kafka
	  SCHEDULED: <2022-11-17 Thu>
		- Should we look into just using streaming delta live tables?
			- They work well and are native in databricks, just don't think anyone looked into it prior
			- Does implement CDF and streams updates
		- ((6373ef69-0137-400f-9181-7835721fa196))
	- May have the new Sam joining me for November phase 2 stuff
- 11:36 - Adding some tasks for [[odp_integration]]
  collapsed:: true
  :LOGBOOK:
  CLOCK: [2022-11-15 Tue 13:46:08]--[2022-11-15 Tue 13:46:13] =>  00:00:05
  :END:
	- TODO move step functions to .py
	  :LOGBOOK:
	  CLOCK: [2022-11-15 Tue 11:39:09]--[2022-11-15 Tue 11:39:10] =>  00:00:01
	  CLOCK: [2022-11-15 Tue 11:39:11]--[2022-11-15 Tue 11:39:12] =>  00:00:01
	  CLOCK: [2022-11-15 Tue 11:39:15]--[2022-11-15 Tue 11:39:15] =>  00:00:00
	  :END:
		- TODO Put together documented workflow on generating module from the .py files
	- TODO Add passphrase support to [[odp_integration/step_function/decrypt_file]]
- 12:57 - Incremental Loading using delta live tables #work
  id:: 6373ef69-0137-400f-9181-7835721fa196
  collapsed:: true
	- More research from the meeting this morning
	- There is a way to connect RDS to S3 to export parquet files directly
		- This would enable us to easily create streaming delta live tables in databricks rather than using debezium and kafka
	- Some helpful links for [[databricks/delta_live_tables]]
		- https://docs.databricks.com/ingestion/auto-loader/options.html
			- Contains arguments to use with the auto loader
		- https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-incremental-data.html#language-python
			- Info on setting up a streaming delta live table
		- https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html
			- Setting up change data capture (cdc) in delta live tables
- 14:00 - Sprint Kickoff
  collapsed:: true
	- All hands meeting [tomorrow]([[20221116]])
	  SCHEDULED: <2022-11-16 Wed>
	- We do have Wednesday - Friday off next week
	  SCHEDULED: <2022-11-23 Wed>
- 14:30 - [[databricks_data_engineering_training]] DE 4.1.2 Customers Pipeline #inbox
  collapsed:: true
	- Builds off of ((63741d60-ed13-4c8c-a07d-c788e6708092)) code
	- # Basic Bronze Pipeline
	  id:: 63741d60-9cae-4d83-99f1-4cf28c1fa434
		- ```sql
		  create or refresh streaming live table customers_bronze
		  comment "Raw data from customers CDC feed"
		  as select current_timestamp() processing_time, input_file_name() source_file, *
		  from cloud_files("${source}/customers", "json")
		  ```
			- Bronze table for streaming data in
			- Uses databricks auto loader
			- In this example, the infer schema option is not specified
				- Since it's pulling from json, all of the column names will be correct
				- They will only be stored as `string` type though
			- Test embed
	- # Quality Enforcement
	  collapsed:: true
		- Rules for ignoring the expected null values from `delete` operations
		- ```sql
		  create streaming live table customers_bronze_clean
		  (
		    constraint valid_id expect (customer_id is not null) on violation fail update,
		    constraint valid_operation expect (operation is not null) on violation drop row,
		    constraint valid_name expect (name is not null or operation = "DELETE"),
		    constraint valid_address expect(
		    	(
		        address is not null
		        and city is not null
		        and state is not null
		        and zip_code is not null
		      ) or operation = "DELETE"
		    ),
		    constraint valid_email expect(
		    	rlike(email, '^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+).([a-zA-Z0-9_\\-\\.]+)')
		      or operation = "DELETE"
		    ) on violation drop row
		  )
		  as select *
		  from stream(live.customers_bronze)
		  ```
			- `fail update`
				- Transaction fails, does not get updated
				- Transaction fails if `customer_id` is null
			- `drop row`
				- Drop any records matching the condition
				- Drops any rows where `operation` is null
				- Drops any rows where `email` does not fit the regex pattern of *@.*.*
			- No `on violation` statement
				- The issue is logged, but the data still gets updated
	- # Processing CDC Data with `Apply Changes Into`
	  collapsed:: true
		- ```sql
		  create or refresh streaming live table customers_silver
		  
		  apply changes into live.customers_silver
		  	from stream(live.customers_bronze_clean)
		      keys (customer_id)
		      apply as delete when operation = "DELETE"
		      SEQUENCE by timestamp
		      columns * except (operation, source_file, _recursed_data)
		  ```
			- Creates streaming live table `customers_silver`
			- Can apply deletes, append, edit
			- Basically merging, including deleting rows
	- # DLT Views
	  collapsed:: true
		- Can be specified as streaming as well (just like DLT tables)
		- Same guarantees as tables
			- Main difference is that the result of queries is not stored on the disk
		- Live views are not persistent in the metastore
			- Only accessible from within the DLT pipeline
			- DONE Check if the views we made for tp
			  SCHEDULED: <2022-11-16 Wed>
- 16:08 - [[databricks_data_engineering_training]] DE 4.1.3 Status Pipeline #inbox
  collapsed:: true
	- Nothing interesting happened here
	- Was just about troubleshooting pipelines when you put some broken notebooks and syntax in there
- 16:21 - [[logseq/block]] features #inbox
  collapsed:: true
	- It is possible to sort sub-blocks (and reverse-sort them)
	- `/sort blocks` on a parent-block sorts all child-blocks
	- `/block embed` is pretty cool too
		- Links to a specific block
		- Shows the whole thing (default is collapsed)
		- You can edit directly in that embed