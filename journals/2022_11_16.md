- 13:00 - All hands #work/meeting
  collapsed:: true
	- Hot ones challenge again
		- #followup Scoville rankings? Seems like high values for not being that spicy
	- #followup Funnel web spiders
	- Danny Recommends going back in time and investing in bitcoin
	- Abby has been here for 7 years
	- Exceptional achievement awards
		- Lotta paralegals and stuff
	- We got the 2022 best lawyers of the year (?) award
	- 5 new partners starting January 2023
		- Partners are for individual clients
			- Oracle
			- Google
			- Amazon
			- etc
	- The LA office is now open and running
		- Looks pretty small, not a big office only ~7 people in the office from what I can see
		- Third office in California
	- Jason Chancellor
		- Says he's above IT so go to him with questions
			- He's not technical though, so... interesting phrasing
		- Refers to the whole reporting and technology fields as IT
	- Nicole Dawson
		- Starts on Monday
		- Chief People Officer
	- EPT with Patrick Gallagher
		- Need to mark off at least one thing in each one on one
		  SCHEDULED: <2022-11-23 Wed>
		- Goals
			- All closing all 2022 goals by December 31st
			- Gotta finish the [[databricks_data_engineering_training and get the certificate
			- 2023 Goals
				- Think about purpose
				- Narrow it down to work and non-work goals
				- Make them "SMART"
					- Specific
					- Measurable
					- Actionable
					- Reasonable
					- Time
	- Bonuses
		- At budget revenue number currently
			- Will be able to do bonuses
	- Economy
		- Initiation (?) is slowing, just need to balance the workload though
			- Normal for end of year
		- Layoffs shouldn't impact us much
			- The big tech companies laying off large amounts of people shouldn't hurt us
			- Even with meta laying off 10,000 people, that's a relatively small amount compared to their overall population
	- Wednesday being off was announced last week
		- Makes sense why our team wasn't heard yet
	- Closed on 12/23 and 12/26 for Christmas
	  SCHEDULED: <2022-12-23 Fri>
- 15:17 - Thoughts on changes to my [[pkm]] [[meta]]
  collapsed:: true
	- I don't think I'm a big fan of the PARA method
		- It isn't really how my brain works
	- I think I will go back to a version of how my obsidian vault was setup
		- Atlas
			- Topic 1
			- Topic 2
				- Idea 1
				- Idea 2
				- Etc
		- Projects
			- Project 1
			- Project 2
				- Related file 1
				- Related file 2
		- Inbox
		- Processing (or some other intermediate word)
		- Atomic (or card, or something else, idk yet)
		-
- 15:22 - databricks_data_engineering_training - DE 4.1.1 Orders Pipeline (python) #archive/inbox
  id:: 6377f51f-3130-46f7-af6e-2fc431ab0186
  collapsed:: true
	- Very similar to ((63756027-fc20-45bd-9c14-7242c0215833)), just for python instead of sql
	- What's needed to make delta live tables in python
	  collapsed:: true
		- ```python
		  import dlt
		  import pyspark.sql.functions
		  ```
	- collapsed:: true
	  ```python
	  @dlt.table
	  def table_name():
	    return {dataframe to become table}
	  ```
		- Basic syntax for creating a delta live table in python
	- collapsed:: true
	  ```python
	  import dlt
	  import pyspark.sql.functions as f
	  
	  @dlt.table
	  def orders_bronze():
	    return(
	    	spark.readStream
	      	.format("cloudFiles")
	      	.option("cloudFiles.format", "json")
	      	.option("cloudFiles.inferColumnTypes", "true")
	      	.load(f"{source}/orders")
	      	.select(
	          	f.current_timestamp().alias("processing_time"),
	            	f.input_file_name().alias("source_file"),
	            	"*"
	          )
	    )
	  ```
		- This is the same table as ((63741d60-9cae-4d83-99f1-4cf28c1fa434))
	- ```python
	  @dlt.table(
	  	comment = "Append only orders with valid timestamp",
	    	table_properties = {"quality": "silver"}
	  )
	  @dlt.expect_or_fail("valid_date", f.col("order_timestamp") > "2021-01-01")
	  def orders_silver():
	    return(
	    	dlt.read_stream("orders_bronze")
	      	.select(
	          	"processing_time",
	            	"customer_id",
	            	"notifications",
	            	"order_id",
	            	f.col("order_timestamp").cast("timestamp").alias("order_timestamp")
	          )
	    )
	  ```
		- Creates a "silver" DLT'
		- Stops the transaction if the timestamp is older than 2021-01-01
	- ```python
	  @dlt.table
	  def orders_by_date():
	    return (
	    	dlt.read("orders_silver")
	      	.groupby(f.col("order_timestamp").cast("date").alias("order_date"))
	      	.agg(f.count("*").alias("total_daily_orders"))
	    )
	  ```
		- Returns a live table of aggregated data
	- Live vs streaming tables
		- Live
			- Contents will always match their definition, even after updates
			- Returns the same result whether it's run for the first time or reloaded
			- Should not be modified by operations outside of the pipeline
		- Streaming
			- Only works with "append-only" streaming sources
			- Only reads input batch once, even if the function changes after reading
			- Can change tables outside of the pipeline
				- append, GDPR, etc
- 15:37 - databricks_data_engineering_training DE 4.1.2 - Customers Pipeline #archive/inbox
  id:: 6377f51f-9033-47ae-9622-2094273e2338
  collapsed:: true
	- collapsed:: true
	  ```python
	  @dlt.table(
	  	name = "customers_bronze",
	    	comment = "Raw data from customers CDC feed"
	  )
	  def ingest_customers_cdc():
	    return(
	    	spark.readStream
	      	.format("cloudFiles")
	      	.option("cloudFiles.format", "json")
	      	.load(f"{source}/customers")
	      	.select(
	          	f.current_timestamp(),
	            	f.input_file_name(),
	            	"*"
	          )
	    )
	  ```
		- All fields will be stored as string since inferSchema was not set
		- The resulting DLT will be called "customers_bronze" instead of "ingest_customers_cdc" because the "name" was set in the @dlt.table call
	- ```python
	  @dlt.table
	  @dlt.expect_or_fail("valid_id", "customer_id is not null")
	  @dlt.expect_or_drop("valid_operation", "operation is not null")
	  @dlt.expect("valid_name", "name is not null or operation = 'DELETE'")
	  @dlt.expect(
	  	"valid_address",
	    	"""
	      (
	      	address is not null
	          and city is not null
	          and state is not null
	          and zip_code is not null
	      ) or operation = "DELETE"
	      """
	  )
	  ```
		- Chaining together multiple types of merge statements for QA
			- Python implementation of ((63756027-7b64-4f86-9590-46ee87d43c3a))
	- ```python
	  dlt.create_target_table(
	  	name = "customers_silver"
	  )
	  
	  dlt.apply_changes(
	  	target = "customers_silver",
	    	source = "customers_bronze_clean",
	    	keys = ["customer_id"],
	    	sequence_by = f.col("timestamp"),
	    	apply_as_deletes = f.expr("operation = 'Delete'")
	    	except_column_list = ["operation", "source_file", "_rescued_data"]
	  )
	  ```
		- Python implementation of ((63756027-8c54-485a-89f7-06a0caec38da))
	- ```python
	  @dlt.view
	  def subscribed_order_emails_vw():
	    return(
	    	dlt.read("orders_silver").filter("notifications = 'Y'").alias("a")
	      	.join(
	          	dlt.read("customers_silver").alias("b"),
	            	on = "customer_id"
	          ).select(
	          	"a.customer_id",
	            	"a.order_id",
	            	"b.email"
	          )
	    )
	  ```
		- Creates a new DLT view
		- Can also inherit streaming execution from the functions they decorate
		- DLT Views do not persist in the metastore
			- Can only be found in the pipeline, not metastore
- 15:50 - Done with databricks training for today, module 5 is up next