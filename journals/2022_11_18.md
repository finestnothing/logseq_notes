- 13:00 - Done registering the [[car]], about $520 which wasn't too bad [[finances]]
- 13:45 - Working on a new [[meta]] for my [[pkm]]
  collapsed:: true
	- There shouldn't be many namespaces
		- Namespaces will be reserved for projects and closely related files to group them better
	- Main linking files
		- Inbox
			- Random thoughts and ideas enter here
			- Raw notes and such
			- Anything that I could want later
		- Staging
			- Notes from inbox go here after I clean them up and rewrite them in my own words
			- Add some new thoughts
			- Add possible sectioning
		- prd_orphans
			- Notes that I don't have any connections to yet
		- Atlas
			- Projects
			- Archive
			- Maps of content
- 15:25 - [[staging]] note on Databricks Delta Live Tables
  collapsed:: true
	- # Data Streaming
	  collapsed:: true
		- ((6377f51f-07b6-4e23-b142-f29b8d06d78d)) are a great way to begin the streaming process
		- Databricks auto loader
		  id:: 637808c7-6124-4ec4-b042-3e29de32e193
			- Built in databricks tool for data streaming
			- Can connect to other data sources to update ((63780818-74c3-483b-9d16-694bdcacb4ec))
			- Still need to look into this more on best cases to use
			- Optimized incremental data loading and streaming
	- # Python and SQL DLT's
	  collapsed:: true
		- ## Basic DLT's
		  collapsed:: true
			- collapsed:: true
			  ```python
			  import dlt
			  
			  @dlt.table
			  def order_bronze():
			    return({spark dataframe query})
			  ```
				- This is the basic syntax for creating a delta live table
			- ```sql
			  create or refresh live table order_bronze
			  tblproperties (
			  	'pipelines.autoOptimize.zOrderCols' = 'case_id'
			  ) as (
			  	{sql query}
			  )
			  ```
		- ## Adding Properties to DLT's
		  collapsed:: true
			- ```python
			  @dlt.table(
			  	comment = "Append only orders with valid timestamp",
			    	table_properties = {"quality": "silver"}
			  )
			  ```
			- ```sql
			  create or refresh live table {table name}
			  comment "Append only orders with valid timestamp"
			  ```
		- ## Streaming DLT's
		  collapsed:: true
			- ```python
			  import dlt
			  import pyspark.sql.functions as f
			  
			  @dlt.table
			  def orders_bronze():
			    return(
			    	spark.readStream
			      	.format("cloudFiles")
			      	.option("cloudFiles.format", "json")
			      	.option("cloudFiles.inferColumnTypes", "true")
			      	.load(f"{source}/orders")
			      	.select(
			          	f.current_timestamp().alias("processing_time"),
			            	f.input_file_name().alias("source_file"),
			            	"*"
			          )
			    )
			  ```
			- ```sql
			  create or refresh streaming live table customers_bronze
			  comment "Raw data from customers CDC feed"
			  as select current_timestamp() processing_time, input_file_name() source_file, *
			  from cloud_files("${source}/customers", "json")
			  ```
			- Both of these utilize ((637808c7-6124-4ec4-b042-3e29de32e193))
		- ## Quality Enforcement in DLT's
		  collapsed:: true
			- ```python
			  @dlt.table
			  @dlt.expect_or_fail("valid_id", "customer_id is not null")
			  @dlt.expect_or_drop("valid_operation", "operation is not null")
			  @dlt.expect("valid_name", "name is not null or operation = 'DELETE'")
			  @dlt.expect(
			  	"valid_address",
			    	"""
			      (
			      	address is not null
			          and city is not null
			          and state is not null
			          and zip_code is not null
			      ) or operation = "DELETE"
			      """
			  )
			  ```
			- ```sql
			  create streaming live table customers_bronze_clean
			  (
			    constraint valid_id expect (customer_id is not null) on violation fail update,
			    constraint valid_operation expect (operation is not null) on violation drop row,
			    constraint valid_name expect (name is not null or operation = "DELETE"),
			    constraint valid_address expect(
			    	(
			        address is not null
			        and city is not null
			        and state is not null
			        and zip_code is not null
			      ) or operation = "DELETE"
			    ),
			    constraint valid_email expect(
			    	rlike(email, '^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+).([a-zA-Z0-9_\\-\\.]+)')
			      or operation = "DELETE"
			    ) on violation drop row
			  )
			  as select *
			  from stream(live.customers_bronze)
			  ```
			- Both of these implement the same constraint checking
			- This is useful for only getting good data
			- #### Constraints
				- `fail update`
					- Transaction fails, does not get updated
					- Transaction fails if `customer_id` is null
				- `drop row`
					- Drop any records matching the condition
					- Drops any rows where `operation` is null
					- Drops any rows where `email` does not fit the regex pattern of *@.*.*
				- No `on violation` statement
					- The issue is logged, but the data still gets updated
		- ## Using `Apply Changes Into` for Incremental Loads
		  collapsed:: true
			- ```python
			  dlt.create_target_table(
			  	name = "customers_silver"
			  )
			  
			  dlt.apply_changes(
			  	target = "customers_silver",
			    	source = "customers_bronze_clean",
			    	keys = ["customer_id"],
			    	sequence_by = f.col("timestamp"),
			    	apply_as_deletes = f.expr("operation = 'Delete'")
			    	except_column_list = ["operation", "source_file", "_rescued_data"]
			  )
			  ```
			- ```sql
			  create or refresh streaming live table customers_silver
			  
			  apply changes into live.customers_silver
			  	from stream(live.customers_bronze_clean)
			      keys (customer_id)
			      apply as delete when operation = "DELETE"
			      SEQUENCE by timestamp
			      columns * except (operation, source_file, _recursed_data)
			  ```
			- Both code blocks create a new streaming live table called customers_silver
			- They then apply changes into the table
				- Databricks keeps track of every change
				- First "apply_changes" will load all of the data in
				- After the first load, only changes will be moved
					- Any updates, additions, and deletions
					- Does not move all of the data over again
			- We also specify that we use customer_id as the primary key
				- Primary key is necessary, multiple can be used
		- # Delta Live Views
		  collapsed:: true
			- Live views in python and databricks follow the same format as tables
				- Only difference is "view" instead of "table
					- `@dlt.view`
					- `create or refresh live view`
			- Views do NOT persist on the disk, they rely on their source tables to continue existing
			- Views are not accessible outside of the pipeline and do not persist in the metastore
	- # Python DLT's
	  collapsed:: true
		- DLT's can be defined in SQL or Python
		- At work, we currently use SQL
			- I tried a python DLT on [[20221118]] but it refused to work
		- ## Code Blocks
			- ```python
			  import dlt
			  ```
				- This may require you to run `pip install dlt` before using
				- DLT is a decorator for python
	- # Streaming Versus Live DLT's
	  collapsed:: true
		- Live tables
			- Basically a persistent view in a data lakehouse format
				- It actually stores and caches the data that is a part of it
					- A view doesn't cache anything, each time you call the view it goes to it's sources
			- If on a triggered pipeline, the data only updates whenever the pipeline is triggered
				- Usually a manual run, but it can be scheduled too
				- This is a tradeoff
					- It is a lower cost and processing time since it runs once and shuts down
					- The data is only as updated as the last pipeline runtime
		- Streaming Live Tables
		  id:: 63780818-74c3-483b-9d16-694bdcacb4ec
			- It is also persistent
			- The main difference between this and ((6377f51f-de60-4ec5-b27b-9145b1df397c)) is that this one stream ingests data
				- Any changes that occur in the source data/table/files/etc is the only thing that gets updated rather than pulling in the entire source each time
				- It updates incrementally so append-only is best, but it can still handle updates and deletes
			- Can have real-time data since it triggers the data update whenever it detects that the source updated
				- This works well for chaining together streaming live tables and views
	- Sources
	  collapsed:: true
		- ((6377f51f-3130-46f7-af6e-2fc431ab0186))
		- ((63741d60-ed13-4c8c-a07d-c788e6708092))
		- ((63756027-fc20-45bd-9c14-7242c0215833))
		- ((6377f51f-9033-47ae-9622-2094273e2338))
- 16:14 - [[staging]] note on some logseq features
  collapsed:: true
	- # Cards
	  collapsed:: true
		- ## Clozes
			- {{cloze This is what a cloze looks like.}}
			- A cloze hides the information inside until clicked (or revealed in the flashcard area)
			- Clozes can be used in regular blocks, but work best in cards
			- To make a cloze, type `/cloze`
		- Any sub-blocks become the content of the card
		- Putting the `#card` tag on a block converts it into a block
			- Other tags can (and should) be added since you can filter the cards in the flashcard side
				- This lets you filter cards to just topics you want to review
	- # Embedding Blocks
	  collapsed:: true
		- `/block embed` embeds an entire block
		- Embedding is essentially making a view pointing to a block and all sub blocks
			- Shows the entire block (it can still be collapsed)
			- You can edit the source block directly from either file
		- Embedding a block links the two notes
	- # Sorting Blocks
	  collapsed:: true
		- `/sort blocks` or `/Reverse Sort Blocks`on a parent block to sort all next-level blocks
		- Does *not* recursively sort
		- You can sort blocks ascending or descending order
			- This clues in to the fact that the order of sub-blocks should not matter, I need to work on this
	- Sources
	  collapsed:: true
		- ((6377f51f-8c99-4d7a-99fb-080610d1f0d8))
		- ((6377f51f-21b7-4919-b054-f4941978d8a7))
- 16:42 - Added stories to [[databricks_data_engineering_training]] to map out the rest of the courses